{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/francesco-vaccari/ProjectDL/blob/fra/Baseline_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKEsG-CKLS4W",
    "outputId": "cbe4abbe-02b2-4e08-be69-c748ac8faf96"
   },
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
    "!tar -xf /content/refcocog.tar.gz\n",
    "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from RefcocogDataset import RefcocogDataset\n",
    "\n",
    "# General Variables\n",
    "REFCOCOG_PATH = \"refcocog\"\n",
    "CLIP_VISUAL_MODEL = \"ViT-B/32\" # this should be ViT-B/16\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yefapNlDMpgC"
   },
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T19:41:20.457779Z",
     "start_time": "2023-05-04T19:41:19.162850Z"
    },
    "id": "8ev_P4ezKqdg"
   },
   "outputs": [],
   "source": [
    "_, preprocess = clip.load(CLIP_VISUAL_MODEL)\n",
    "\n",
    "train_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"train\", transform=preprocess, tokenization=clip.tokenize)\n",
    "val_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"val\", transform=preprocess, tokenization=clip.tokenize)\n",
    "test_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"test\", transform=preprocess, tokenization=clip.tokenize)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "train_features, train_bbox = next(iter(train_dataloader))\n",
    "train_dataset.showImage(train_features, train_bbox)\n",
    "# print(train_dataset.getSentences(train_features))\n",
    "# print(len(train_dataset))\n",
    "# print(len(val_dataset))\n",
    "# print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwzN7Au1MmgX"
   },
   "source": [
    "## BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T19:50:00.729239Z",
     "start_time": "2023-05-04T19:50:00.725248Z"
    },
    "id": "SM65Q7Y9bXXu"
   },
   "outputs": [],
   "source": [
    "def YoloBBoxes(img, modelYOLO):\n",
    "    result = modelYOLO(img)\n",
    "    # result.show()\n",
    "    bbox = result.pandas().xyxy[0]\n",
    "    bbox = bbox.reset_index()\n",
    "    bbox[\"tconfidence\"] = np.nan\n",
    "    bbox[\"crop\"] = np.nan\n",
    "    return bbox\n",
    "\n",
    "def CropImage(image, boxs):\n",
    "    crops = []\n",
    "    for index, row in boxs.iterrows():\n",
    "        box = (\n",
    "            row['xmin'],\n",
    "            row['ymin'],\n",
    "            row['xmax'],\n",
    "            row['ymax'],\n",
    "        )\n",
    "        crop = image.crop(box)\n",
    "        crops.append(crop)\n",
    "        boxs.at[index, 'crop'] = crop\n",
    "    return crops\n",
    "\n",
    "def computeSimilarity(image, sentences, modelCLIP, preprocessCLIP):\n",
    "    similarities = []\n",
    "    for sent in sentences:\n",
    "        with torch.no_grad():\n",
    "            image_features = modelCLIP.encode_image(torch.unsqueeze(preprocessCLIP(image).to(device), dim=0)).float().to(device)\n",
    "            text_features = modelCLIP.encode_text(sent[0]).float().to(device)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarities.append(text_features.cpu().numpy() @ image_features.cpu().numpy().T)\n",
    "    return sum(similarities)/len(similarities)\n",
    "\n",
    "def computeIntersection(fx1, fy1, fx2, fy2, sx1, sy1, sx2, sy2):\n",
    "    dx = min(fx2, sx2) - max(fx1, sx1)\n",
    "    dy = min(fy2, sy2) - max(fy1, sy1)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        area = dx*dy\n",
    "    else:\n",
    "        area = 0\n",
    "    return area\n",
    "\n",
    "def computeAccuracy(bboxes, index, label):\n",
    "    intersection = computeIntersection(bboxes['xmin'][index], bboxes['ymin'][index], bboxes['xmax'][index], bboxes['ymax'][index], \n",
    "                                       label[0].item(), label[1].item(), label[0].item()+label[2].item(), label[1].item()+label[3].item())\n",
    "    area1 = (bboxes['xmax'][index]-bboxes['xmin'][index])*(bboxes['ymax'][index]-bboxes['ymin'][index])\n",
    "    area2 = label[2].item()*label[3].item()\n",
    "    union = area1 + area2 - intersection\n",
    "    return intersection / union\n",
    "\n",
    "def baseline(loader, dataset, modelYOLO, modelCLIP, preprocessCLIP):\n",
    "    n_samples = 0\n",
    "    tot_accuracy = 0\n",
    "    for data_features, data_bbox in loader:\n",
    "\n",
    "        bboxes = YoloBBoxes(dataset.getImage(data_features), modelYOLO)\n",
    "        crops = CropImage(dataset.getImage(data_features), bboxes)\n",
    "\n",
    "        if len(crops) > 0:\n",
    "            highest_similarity = 0\n",
    "            index_pred = 0\n",
    "            for i, crop in enumerate(crops):\n",
    "                similarity = computeSimilarity(crop, data_features['sentences'], modelCLIP, preprocessCLIP)\n",
    "                if similarity > highest_similarity:\n",
    "                    highest_similarity = similarity\n",
    "                    index_pred = i\n",
    "        \n",
    "            accuracy = computeAccuracy(bboxes, index_pred, data_bbox)\n",
    "        else:\n",
    "            accuracy = 0\n",
    "        \n",
    "        tot_accuracy += accuracy\n",
    "        n_samples += 1\n",
    "        print(f'Image {n_samples:^8}/{len(dataset):^8}\\t{accuracy}')\n",
    "\n",
    "    return tot_accuracy/n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "kpB9el1rb1cA",
    "outputId": "ec732aca-562c-4dd4-c064-d9044e5e5935"
   },
   "outputs": [],
   "source": [
    "modelYOLO = torch.hub.load('ultralytics/yolov5', 'yolov5x')\n",
    "modelCLIP, preprocessCLIP = clip.load(CLIP_VISUAL_MODEL, device=device)\n",
    "\n",
    "baseline(test_dataloader, test_dataset, modelYOLO, modelCLIP, preprocessCLIP)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMova2zu5wekBOzUIodq1UJ",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
