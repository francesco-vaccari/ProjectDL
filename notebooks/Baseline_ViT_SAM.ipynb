{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnujRuLP78Mc"
      },
      "source": [
        "# Baseline with Clip ViT 16 and SAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huAlPL0AjfmC"
      },
      "outputs": [],
      "source": [
        "# CLIP\n",
        "!git clone https://github.com/hila-chefer/Transformer-MM-Explainability\n",
        "import os\n",
        "os.chdir(f'./Transformer-MM-Explainability')\n",
        "!pip install einops\n",
        "!pip install ftfy\n",
        "import CLIP.clip as clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZOMTLNH7_9n"
      },
      "outputs": [],
      "source": [
        "# REFCOCOG\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!tar -xf /content/refcocog.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtEr-9A08SIz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import locale\n",
        "\n",
        "# from typing import Sequence, Union\n",
        "\n",
        "locale.getpreferredencoding = lambda: 'UTF-8'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-v6Prec8WE5"
      },
      "outputs": [],
      "source": [
        "# SAM\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzfEmm4h8amX"
      },
      "outputs": [],
      "source": [
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\"\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "predictor = SamPredictor(sam)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZIfbrgG8hNa"
      },
      "outputs": [],
      "source": [
        "class RefcocogDataset(Dataset):\n",
        "    def __init__(self, base_path, split=None, transform=None, tokenization=None):\n",
        "        annotation_path = base_path + \"/annotations/\"\n",
        "\n",
        "        self.IMAGES_PATH = base_path + \"/images/\"\n",
        "        self.transform = transform\n",
        "        self.tokenization = tokenization\n",
        "\n",
        "        tmp_annotations = pandas.read_pickle(annotation_path + \"refs(umd).p\")\n",
        "        tmp_instances = json.load(open(annotation_path + \"instances.json\", \"r\"))\n",
        "\n",
        "        annotations_dt = pandas.DataFrame.from_records(tmp_annotations) \\\n",
        "            .filter(items=[\"image_id\", \"split\", \"sentences\", \"ann_id\"])\n",
        "\n",
        "        instances_dt = pandas.DataFrame.from_records(tmp_instances['annotations'])\n",
        "\n",
        "        self.annotations = annotations_dt \\\n",
        "            .merge(instances_dt[[\"id\", \"bbox\", \"area\"]], left_on=\"ann_id\", right_on=\"id\") \\\n",
        "            .drop(columns=\"id\")\n",
        "\n",
        "        if split is not None:\n",
        "            self.annotations = self.__get_annotations_by_split(split.lower())\n",
        "\n",
        "    def getImage(self, sample):\n",
        "        id = sample['idx'][0].item()\n",
        "        item = self.annotations.iloc[id]\n",
        "        image = self.__getimage(item.image_id)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def getSentences(self, sample):\n",
        "        id = sample['idx'][0].item()\n",
        "        item = self.annotations.iloc[id]\n",
        "\n",
        "        return self.__extract_sentences(item.sentences)\n",
        "\n",
        "    def showImage(self, train_features, train_bbox):\n",
        "        img = self.getImage(train_features)\n",
        "        img1 = ImageDraw.Draw(img)\n",
        "        img1.rectangle([(train_bbox[0].item(), train_bbox[1].item()), (train_bbox[2].item(), train_bbox[3].item())], outline =\"red\")\n",
        "        img.show()\n",
        "\n",
        "    def __get_annotations_by_split(self, split):\n",
        "        return self.annotations[self.annotations.split == split].reset_index()\n",
        "\n",
        "    def __getimage(self, id):\n",
        "        return Image.open(self.IMAGES_PATH + \"COCO_train2014_\" + str(id).zfill(12) + \".jpg\")\n",
        "\n",
        "    def __extract_sentences(self, sentences):\n",
        "        return [f\"a photo of {s['sent']}\" for s in sentences]\n",
        "\n",
        "    def __tokenize_sents(self, sentences):\n",
        "        return [self.tokenization(s) for s in sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.annotations.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.annotations.iloc[idx]\n",
        "        image = self.__getimage(item.image_id)\n",
        "        sentences = self.__extract_sentences(item.sentences)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.tokenization:\n",
        "            sentences = self.__tokenize_sents(sentences)\n",
        "\n",
        "        sample = {'idx': idx, 'image': image, 'sentences': sentences}\n",
        "\n",
        "        return sample, item.bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVTFCJfH8yCx"
      },
      "outputs": [],
      "source": [
        "def computeIntersection(bbox, BBOX):\n",
        "  x_min = bbox[0]\n",
        "  y_min = bbox[1]\n",
        "  x_max = bbox[2]\n",
        "  y_max = bbox[3]\n",
        "  X_MIN = BBOX[0]\n",
        "  Y_MIN = BBOX[1]\n",
        "  X_MAX = BBOX[2]\n",
        "  Y_MAX = BBOX[3]\n",
        "\n",
        "  dx = min(x_max, X_MAX) - max(x_min, X_MIN)\n",
        "  dy = min(y_max, Y_MAX) - max(y_min, Y_MIN)\n",
        "  if (dx>=0) and (dy>=0):\n",
        "      area = dx*dy\n",
        "  else:\n",
        "      area = 0\n",
        "  return area\n",
        "\n",
        "def computeAccuracy(bbox, BBOX):\n",
        "    intersection = computeIntersection(bbox, BBOX)\n",
        "    area1 = (bbox[2]-bbox[0])*(bbox[3]-bbox[1])\n",
        "    area2 = (BBOX[2]-BBOX[0])*(BBOX[3]-BBOX[1])\n",
        "    union = area1 + area2 - intersection\n",
        "    return intersection / union\n",
        "\n",
        "def cosine_similarity(images_z: torch.Tensor, texts_z: torch.Tensor):\n",
        "  # normalise the image and the text\n",
        "  images_z /= images_z.norm(dim=-1, keepdim=True)\n",
        "  texts_z /= texts_z.norm(dim=-1, keepdim=True)\n",
        "\n",
        "  # evaluate the cosine similarity between the sets of features\n",
        "  similarity = (texts_z @ images_z.T)\n",
        "\n",
        "  return similarity.cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5smuH7l88j0"
      },
      "outputs": [],
      "source": [
        "def point_from_heat_map(heat_map, visualize=False):\n",
        "\n",
        "  A = np.zeros((224,224))\n",
        "  for i in range(224):\n",
        "    for j in range(224):\n",
        "      if heat_map[i][j] < 0.50:\n",
        "        A[i][j] = 0.0\n",
        "      else:\n",
        "        A[i][j] = 1.0\n",
        "\n",
        "  if visualize==True:\n",
        "    plt.imshow(A)\n",
        "\n",
        "  center = [0, 0]\n",
        "  xs = []\n",
        "  ys = []\n",
        "  flag = False\n",
        "\n",
        "  for i in range(224):\n",
        "    for j in range(224):\n",
        "      if A[i][j] == 1:\n",
        "        xs.append(i)\n",
        "        ys.append(j)\n",
        "        flag = True\n",
        "\n",
        "  if flag:\n",
        "    center[0] = int((max(xs)+min(xs))/2)\n",
        "    center[1] = int((max(ys)+min(ys))/2)\n",
        "\n",
        "  return center"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "916RHmTtAqk4"
      },
      "outputs": [],
      "source": [
        "start_layer =  -1\n",
        "start_layer_text =  -1\n",
        "\n",
        "def interpret(image, texts, model, device, start_layer=start_layer, start_layer_text=start_layer_text):\n",
        "    batch_size = texts.shape[0]\n",
        "    images = image.repeat(batch_size, 1, 1, 1)\n",
        "    logits_per_image, logits_per_text = model(images, texts)\n",
        "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
        "    index = [i for i in range(batch_size)]\n",
        "    one_hot = np.zeros((logits_per_image.shape[0], logits_per_image.shape[1]), dtype=np.float32)\n",
        "    one_hot[torch.arange(logits_per_image.shape[0]), index] = 1\n",
        "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "    one_hot = torch.sum(one_hot.cuda() * logits_per_image)\n",
        "    model.zero_grad()\n",
        "\n",
        "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
        "\n",
        "    if start_layer == -1:\n",
        "      # calculate index of last layer\n",
        "      start_layer = len(image_attn_blocks) - 1\n",
        "\n",
        "    num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
        "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
        "    R = R.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
        "    for i, blk in enumerate(image_attn_blocks):\n",
        "        if i < start_layer:\n",
        "          continue\n",
        "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
        "        cam = blk.attn_probs.detach()\n",
        "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "        cam = grad * cam\n",
        "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
        "        cam = cam.clamp(min=0).mean(dim=1)\n",
        "        R = R + torch.bmm(cam, R)\n",
        "    image_relevance = R[:, 0, 1:]\n",
        "\n",
        "\n",
        "    text_attn_blocks = list(dict(model.transformer.resblocks.named_children()).values())\n",
        "\n",
        "    if start_layer_text == -1:\n",
        "      # calculate index of last layer\n",
        "      start_layer_text = len(text_attn_blocks) - 1\n",
        "\n",
        "    num_tokens = text_attn_blocks[0].attn_probs.shape[-1]\n",
        "    R_text = torch.eye(num_tokens, num_tokens, dtype=text_attn_blocks[0].attn_probs.dtype).to(device)\n",
        "    R_text = R_text.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
        "    for i, blk in enumerate(text_attn_blocks):\n",
        "        if i < start_layer_text:\n",
        "          continue\n",
        "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
        "        cam = blk.attn_probs.detach()\n",
        "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "        cam = grad * cam\n",
        "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
        "        cam = cam.clamp(min=0).mean(dim=1)\n",
        "        R_text = R_text + torch.bmm(cam, R_text)\n",
        "    text_relevance = R_text\n",
        "\n",
        "    return text_relevance, image_relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlYjK5sZAYRF"
      },
      "outputs": [],
      "source": [
        "def get_image_relevance(image_relevance):\n",
        "\n",
        "    dim = int(image_relevance.numel() ** 0.5)\n",
        "    image_relevance = image_relevance.reshape(1, 1, dim, dim)\n",
        "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
        "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
        "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
        "\n",
        "    return image_relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcQXafhN9Au_"
      },
      "outputs": [],
      "source": [
        "def baseline(loader, dataset, model, preprocess):\n",
        "\n",
        "  n_samples = 0\n",
        "  tot_accuracy = 0\n",
        "  tot_correct = 0\n",
        "\n",
        "  for data_features, data_bbox in loader:\n",
        "\n",
        "    sentences = dataset.getSentences(data_features)\n",
        "\n",
        "    target_bbox = [0, 0, 0, 0]\n",
        "    target_bbox[0] = int(data_bbox[1].item())\n",
        "    target_bbox[1] = int(data_bbox[0].item())\n",
        "    target_bbox[2] = int(data_bbox[1].item() + data_bbox[3].item())\n",
        "    target_bbox[3] = int(data_bbox[2].item() + data_bbox[0].item())\n",
        "\n",
        "    image = dataset.getImage(data_features)\n",
        "\n",
        "    img = np.array(image)\n",
        "    predictor.set_image(img)\n",
        "\n",
        "    points = []\n",
        "    for target in sentences:\n",
        "      image_preprocessed = preprocess(image).unsqueeze(0).to(device)\n",
        "      texts = [target]\n",
        "      text = clip.tokenize(texts).to(device)\n",
        "      R_text, R_image = interpret(model=model, image=image_preprocessed, texts=text, device=device)\n",
        "      heat_map = get_image_relevance(R_image[0])\n",
        "      point = point_from_heat_map(heat_map, visualize=False)\n",
        "      point = [int(point[1]*img.shape[1]/224), int(point[0]*img.shape[0]/224)]\n",
        "      points.append(point)\n",
        "\n",
        "    input_point = np.array(points)\n",
        "    input_label = np.ones(len(sentences))\n",
        "\n",
        "    masks, scores, _ = predictor.predict(\n",
        "        point_coords=input_point,\n",
        "        point_labels=input_label,\n",
        "        multimask_output=True,\n",
        "    )\n",
        "\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    for i in range(masks.shape[1]):\n",
        "      for j in range(masks.shape[2]):\n",
        "        if masks[0][i][j]*1+masks[1][i][j]*1+masks[2][i][j]*1>0:\n",
        "          xs.append(i)\n",
        "          ys.append(j)\n",
        "\n",
        "    bbox = [min(xs), min(ys), max(xs), max(ys)]\n",
        "\n",
        "    # COMPUTE ACCURACY\n",
        "    accuracy = computeAccuracy(bbox, target_bbox)\n",
        "\n",
        "    n_samples += 1\n",
        "    tot_accuracy += accuracy\n",
        "    if accuracy >= 0.5:\n",
        "      tot_correct += 1\n",
        "\n",
        "    print('Correct_classification:', tot_correct/(n_samples), 'TOT_Accuracy:', accuracy)\n",
        "\n",
        "    if n_samples%100==0:\n",
        "      print('\\n-----------------------------------------------------------------')\n",
        "      print('Correct_classification:', tot_correct/(n_samples), 'TOT_Accuracy:', tot_accuracy/(n_samples))\n",
        "      print('\\n-----------------------------------------------------------------')\n",
        "\n",
        "  return #tot_accuracy/n_samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVvsnXBAssWv"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZm1d_-i8pty"
      },
      "outputs": [],
      "source": [
        "REFCOCOG_PATH = \"/content/refcocog\"\n",
        "\n",
        "train_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"train\", transform=preprocess, tokenization=clip.tokenize)\n",
        "val_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"val\", transform=preprocess, tokenization=clip.tokenize)\n",
        "test_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"test\", transform=preprocess, tokenization=clip.tokenize)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-T471448ltR"
      },
      "outputs": [],
      "source": [
        "baseline(test_dataloader, test_dataset, model, preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRdDGgrR-q_i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
