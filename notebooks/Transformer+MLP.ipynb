{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdAc8de4jdFgZdtswZ/8Kk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francesco-vaccari/ProjectDL/blob/main/Transformer%2BMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloads"
      ],
      "metadata": {
        "id": "yzuYxyziRoD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!tar -xf /content/refcocog.tar.gz\n",
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2mX_CmzRnQ5",
        "outputId": "3c07f1a2-5600-4198-da7c-0124bf6ad97e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7l2z9d4g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7l2z9d4g\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369370 sha256=9968d157d40a51e9d6ebf63e42f389dadc8f842c340ca7954bfb2b7024956ac1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1rqh5tuv/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [02:21<00:00, 95.1MB/s]\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m625.9/625.9 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "aIWoA7K0QgNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import clip\n",
        "import torch\n",
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Sequence, Union\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "\n",
        "class RefcocogDataset(Dataset):\n",
        "    def __init__(self, base_path, split=None, transform=None, tokenization=None):\n",
        "        annotation_path = base_path + \"/annotations/\"\n",
        "\n",
        "        self.IMAGES_PATH = base_path + \"/images/\"\n",
        "        self.transform = transform\n",
        "        self.tokenization = tokenization\n",
        "\n",
        "        tmp_annotations = pandas.read_pickle(annotation_path + \"refs(umd).p\")\n",
        "        tmp_instances = json.load(open(annotation_path + \"instances.json\", \"r\"))\n",
        "\n",
        "        annotations_dt = pandas.DataFrame.from_records(tmp_annotations) \\\n",
        "            .filter(items=[\"image_id\", \"split\", \"sentences\", \"ann_id\"])\n",
        "\n",
        "        instances_dt = pandas.DataFrame.from_records(tmp_instances['annotations'])\n",
        "\n",
        "        self.annotations = annotations_dt \\\n",
        "            .merge(instances_dt[[\"id\", \"bbox\", \"area\"]], left_on=\"ann_id\", right_on=\"id\") \\\n",
        "            .drop(columns=\"id\")\n",
        "\n",
        "        if split is not None:\n",
        "            self.annotations = self.__get_annotations_by_split(split.lower())\n",
        "\n",
        "    def getImage(self, sample):\n",
        "        id = sample['idx'][0].item()\n",
        "        item = self.annotations.iloc[id]\n",
        "        image = self.__getimage(item.image_id)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def getSentences(self, sample):\n",
        "        id = sample['idx'][0].item()\n",
        "        item = self.annotations.iloc[id]\n",
        "\n",
        "        return self.__extract_sentences(item.sentences)\n",
        "\n",
        "    def showImage(self, train_features, train_bbox):\n",
        "        img = self.getImage(train_features)\n",
        "        img1 = ImageDraw.Draw(img)\n",
        "        img1.rectangle([(train_bbox[0].item(), train_bbox[1].item()), (train_bbox[0].item()+train_bbox[2].item(), train_bbox[1].item()+train_bbox[3].item())], outline =\"red\")\n",
        "        img.show()\n",
        "\n",
        "    def __get_annotations_by_split(self, split):\n",
        "        return self.annotations[self.annotations.split == split].reset_index()\n",
        "\n",
        "    def __getimage(self, id):\n",
        "        return Image.open(self.IMAGES_PATH + \"COCO_train2014_\" + str(id).zfill(12) + \".jpg\")\n",
        "\n",
        "    def __extract_sentences(self, sentences):\n",
        "        return [f\"a photo of {s['sent']}\" for s in sentences]\n",
        "\n",
        "    def __tokenize_sents(self, sentences):\n",
        "        return [self.tokenization(s) for s in sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.annotations.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.annotations.iloc[idx]\n",
        "        image = self.__getimage(item.image_id)\n",
        "        sentences = self.__extract_sentences(item.sentences)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.tokenization:\n",
        "            sentences = self.__tokenize_sents(sentences)\n",
        "\n",
        "        sample = {'idx': idx, 'image': image, 'sentences': sentences}\n",
        "\n",
        "        return sample, item.bbox"
      ],
      "metadata": {
        "id": "QJstbfWNR3Ha"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, preprocess = clip.load(\"RN50\")\n",
        "REFCOCOG_PATH = \"refcocog\"\n",
        "\n",
        "train_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"train\", transform=preprocess, tokenization=clip.tokenize)\n",
        "val_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"val\", transform=preprocess, tokenization=clip.tokenize)\n",
        "test_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"test\", transform=preprocess, tokenization=clip.tokenize)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6amj_5_GR86g",
        "outputId": "00fd1b5a-02b1-4417-a16c-32d6861ddde4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244M/244M [00:02<00:00, 120MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "-gDXlHynQjbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(2048, 1024)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "E8PgB7xmQxzS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "modelYOLO = torch.hub.load('ultralytics/yolov5', 'yolov5x')\n",
        "modelCLIP, preprocessCLIP = clip.load(\"RN50\", device=device)\n",
        "customModel = CustomModel().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9Eke0MYMJv3",
        "outputId": "503f019d-3268-400d-fe78-4bc2d00fc97f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2023-7-7 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "6TEdQGCSQl5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_crops(image):\n",
        "    results = modelYOLO(image)\n",
        "    crops = []\n",
        "    crops_bboxes = []\n",
        "\n",
        "    for result in results.xyxy[0]:\n",
        "        crops.append(image.crop(result[:4].tolist()))\n",
        "        crops_bboxes.append(result[:4].tolist())\n",
        "\n",
        "    return crops, crops_bboxes\n",
        "\n",
        "def convert_bbox_from_xywh_to_xyxy(bbox):\n",
        "    return [\n",
        "        bbox[0],\n",
        "        bbox[1],\n",
        "        bbox[0] + bbox[2],\n",
        "        bbox[1] + bbox[3]\n",
        "    ]\n",
        "\n",
        "def intersection_over_union(bbox1, bbox2):\n",
        "    x_left = max(bbox1[0], bbox2[0])\n",
        "    y_top = max(bbox1[1], bbox2[1])\n",
        "    x_right = min(bbox1[2], bbox2[2])\n",
        "    y_bottom = min(bbox1[3], bbox2[3])\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "    bbox1_area = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n",
        "    bbox2_area = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n",
        "\n",
        "    iou = intersection_area / float(bbox1_area + bbox2_area - intersection_area)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def get_labels(crops, crops_bboxes, bbox, image):\n",
        "    bbox = [n.item() for n in bbox]\n",
        "    bbox = convert_bbox_from_xywh_to_xyxy(bbox)\n",
        "    labels = torch.zeros(len(crops)).to(device)\n",
        "\n",
        "    found_crop = False\n",
        "    for i, crop_bbox in enumerate(crops_bboxes):\n",
        "        if(intersection_over_union(bbox, crop_bbox) > 0.5):\n",
        "            labels[i] = 1\n",
        "            found_crop = True\n",
        "            break\n",
        "\n",
        "    if not found_crop:\n",
        "        crops.append(image.crop(bbox))\n",
        "        crops_bboxes.append(bbox)\n",
        "        labels = torch.cat((labels, torch.tensor([1]).to(device)))\n",
        "\n",
        "    return crops, crops_bboxes, labels\n",
        "\n",
        "def get_embeddings(preprocessed_image, crops):\n",
        "    image_embedding = modelCLIP.encode_image(preprocessed_image).squeeze().float()\n",
        "\n",
        "    embeddings = []\n",
        "    for crop in crops:\n",
        "        crop_preprocessed = preprocessCLIP(crop).unsqueeze(0).to(device)\n",
        "        crop_embedding = modelCLIP.encode_image(crop_preprocessed).squeeze().float()\n",
        "        embedding = torch.cat((image_embedding, crop_embedding))\n",
        "        embeddings.append(embedding.to(device))\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "def pass_embeddings_through_model(embeddings):\n",
        "    outputs = []\n",
        "    for embedding in embeddings:\n",
        "        output = customModel(embedding)\n",
        "        outputs.append(output)\n",
        "    return outputs\n",
        "\n",
        "def compute_loss(outputs, labels, sentences):\n",
        "    # I want to maximize the similarity between the target vector and the outputs with label 1\n",
        "    # and minimize the similarity between the target vector and the outputs with label 0\n",
        "    loss = 0\n",
        "    for sent in sentences:\n",
        "        sent = sent.to(device)\n",
        "        target_vector = modelCLIP.encode_text(sent[0]).squeeze().to(device)\n",
        "        for i, output in enumerate(outputs):\n",
        "            if labels[i] == 1:\n",
        "                loss -= torch.cosine_similarity(output, target_vector, dim=0)\n",
        "            else:\n",
        "                loss += torch.cosine_similarity(output, target_vector, dim=0)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "7l9wad00QyQf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main loop"
      ],
      "metadata": {
        "id": "-yaKIYvQQowa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = val_dataloader\n",
        "dataset = val_dataset\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(customModel.parameters(), lr=learning_rate)\n",
        "\n",
        "for data_features, data_bbox in loader:\n",
        "\n",
        "    crops, crops_bboxes = extract_crops(dataset.getImage(data_features))\n",
        "\n",
        "    crops, crops_bboxes, labels = get_labels(crops, crops_bboxes, data_bbox, dataset.getImage(data_features))\n",
        "\n",
        "    intput_embeddings = get_embeddings(data_features['image'].to(device), crops)\n",
        "\n",
        "    output_embeddings = pass_embeddings_through_model(intput_embeddings)\n",
        "\n",
        "    loss = compute_loss(output_embeddings, labels, data_features['sentences'])\n",
        "\n",
        "    print(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SA5U4guQywQ",
        "outputId": "dc283ef3-29e7-48ba-e2f8-4a587a35f4e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.83568, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}