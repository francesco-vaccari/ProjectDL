{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Import Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import clip\n",
    "import json\n",
    "import torch\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageDraw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_PATH = \"../refcocog/\"\n",
    "BASE_IMG = BASE_PATH + \"images/\"\n",
    "annotations = pandas.read_pickle(BASE_PATH + \"annotations/refs(umd).p\")\n",
    "\n",
    "ann_dt = pandas.DataFrame.from_records(annotations).filter(items=[\"image_id\", \"split\", \"sentences\", \"ann_id\"])\n",
    "display(ann_dt[ann_dt.split == 'train'])\n",
    "\n",
    "instances = json.load(open(BASE_PATH + \"annotations/instances.json\", 'r'))\n",
    "print(instances.keys())\n",
    "instances_dt = pandas.DataFrame.from_records(instances['annotations'])\n",
    "display(instances_dt)\n",
    "\n",
    "images_dt = pandas.DataFrame.from_records(instances['images'])\n",
    "display(images_dt)\n",
    "\n",
    "\n",
    "train_ann = [ann for ann in annotations if ann['split'] == 'train']\n",
    "test_ann = [ann for ann in annotations if ann['split'] == 'test']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Merge two pandas dataframe to obtain a single dataframe with all the information we need to run all the computations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_dt = ann_dt.merge(instances_dt[[\"id\", \"bbox\", \"area\"]], left_on=\"ann_id\", right_on=\"id\")\n",
    "display(a_dt.head(1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T11:27:34.847758Z",
     "end_time": "2023-04-29T11:27:34.990990Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run YOLO prediction on image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load YOLO model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5x')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T10:31:20.387718Z",
     "end_time": "2023-04-29T10:31:24.006796Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def YoloBBoxes(img):\n",
    "    result = model(img)\n",
    "    result.show()\n",
    "    bbox = result.pandas().xyxy[0]\n",
    "    bbox = bbox.reset_index()\n",
    "    bbox[\"tconfidence\"] = np.nan\n",
    "    bbox[\"crop\"] = np.nan\n",
    "    return bbox\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T10:31:24.009093Z",
     "end_time": "2023-04-29T10:31:24.010645Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CropImage(image, boxs):\n",
    "    crops = []\n",
    "\n",
    "    for index, row in boxs.iterrows():\n",
    "        box = (\n",
    "            row['xmin'],\n",
    "            row['ymin'],\n",
    "            row['xmax'],\n",
    "            row['ymax'],\n",
    "        )\n",
    "\n",
    "        crop = image.crop(box)\n",
    "\n",
    "        crops.append(crop)\n",
    "        boxs.at[index, 'crop'] = crop\n",
    "\n",
    "    return crops\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T10:31:24.012289Z",
     "end_time": "2023-04-29T10:31:24.013137Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute text similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "modelCLIP, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T10:31:24.014396Z",
     "end_time": "2023-04-29T10:31:26.109772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ExtractSent(annotation):\n",
    "    return [f\"a photo of a {s['sent']}\" for s in annotation['sentences']]\n",
    "\n",
    "def ClipSimilarity(image, text):\n",
    "    image_features = modelCLIP.encode_image(image).float()\n",
    "    text_features = modelCLIP.encode_text(text).float()\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = image_features @ text_features.T\n",
    "    return similarity\n",
    "\n",
    "def ComputeTextSimilarity(c, boxes):\n",
    "    for index, row in boxes.iterrows():\n",
    "        # display(c[index])\n",
    "        text_simils = []\n",
    "\n",
    "        for sent in test_sent:\n",
    "            image = preprocess(row['crop']).unsqueeze(0).to(device)\n",
    "            text = clip.tokenize(sent).to(device)\n",
    "\n",
    "            text_simils.append(ClipSimilarity(image, text).detach().numpy())\n",
    "\n",
    "        boxes.at[index, \"tconfidence\"] = (np.array(text_simils).max())\n",
    "\n",
    "def ExtractBestMatch(boxes):\n",
    "    return boxes[boxes.tconfidence == boxes.tconfidence.max()]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T10:31:26.112442Z",
     "end_time": "2023-04-29T10:31:26.113686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CalculateIntersectionArea(fx1, fy1, fx2, fy2, sx1, sy1, sx2, sy2):\n",
    "    print(fx1, fy1, fx2, fy2, sx1, sy1, sx2, sy2)\n",
    "    dx = min(fx2, sx2) - max(fx1, sx1)\n",
    "    dy = min(fy2, sy2) - max(fy1, sy1)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        area = dx*dy\n",
    "    else:\n",
    "        area = 0\n",
    "    return area\n",
    "\n",
    "def VisualizeIntersections(image, best, ann):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    draw.rectangle(\n",
    "        [best_match.xmin, best_match.ymin, best_match.xmax, best_match.ymax],\n",
    "        outline=\"red\",\n",
    "        width=3\n",
    "    )\n",
    "    draw.rectangle(\n",
    "        [bbox_annotation[0], bbox_annotation[1], bbox_annotation[0] + bbox_annotation[2], bbox_annotation[1] + bbox_annotation[3]],\n",
    "        outline=\"blue\",\n",
    "        width=3\n",
    "    )\n",
    "    display(image)\n",
    "\n",
    "def CalculateIntersection(box, annotation):\n",
    "    return\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T11:37:45.862311Z",
     "end_time": "2023-04-29T11:37:45.863674Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = a_dt.head(1)\n",
    "\n",
    "test_img = Image.open(BASE_IMG + \"COCO_train2014_\" + str(img['image_id'].values[0]).zfill(12) + \".jpg\")\n",
    "\n",
    "bbox = YoloBBoxes(test_img)\n",
    "crops = CropImage(test_img, bbox)\n",
    "\n",
    "test_sent = ExtractSent(test_ann[0])\n",
    "ComputeTextSimilarity(crops, bbox)\n",
    "display(bbox)\n",
    "\n",
    "best_match = ExtractBestMatch(bbox)\n",
    "\n",
    "bbox_annotation = img['bbox'].values[0]\n",
    "display(bbox_annotation)\n",
    "\n",
    "print(\"Best BBox Match\")\n",
    "display(best_match)\n",
    "display(best_match['crop'][1])\n",
    "\n",
    "VisualizeIntersections(test_img, best_match, bbox_annotation)\n",
    "\n",
    "area = CalculateIntersectionArea(\n",
    "    best_match.xmin.values[0], best_match.ymax.values[0], best_match.xmax.values[0], best_match.ymin.values[0],\n",
    "    bbox_annotation[0], bbox_annotation[1], bbox_annotation[0] + bbox_annotation[2], bbox_annotation[1] + bbox_annotation[3]\n",
    ")\n",
    "\n",
    "print(area)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T11:38:33.912370Z",
     "end_time": "2023-04-29T11:38:35.742520Z"
    }
   }
  }
 ]
}
