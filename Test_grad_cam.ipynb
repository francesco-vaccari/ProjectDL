{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francesco-vaccari/ProjectDL/blob/fra/Test_grad_cam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq41_i8uis2u",
        "outputId": "3ac5b0d9-a33e-4726-baa7-c1753ffd01f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-gze0scv4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-gze0scv4\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.1+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [02:05<00:00, 107MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!tar -xf /content/refcocog.tar.gz\n",
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klTD08CGQkRe",
        "outputId": "b5f37819-07dc-4c69-d89e-1c6c58afcdca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5MUf1dQiyTo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import clip\n",
        "import torch\n",
        "import pandas\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Sequence, Union\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "\n",
        "class RefcocogDataset(Dataset):\n",
        "    def __init__(self, base_path, split=None, transform=None, tokenization=None):\n",
        "        annotation_path = base_path + \"/annotations/\"\n",
        "\n",
        "        self.IMAGES_PATH = base_path + \"/images/\"\n",
        "        self.transform = transform\n",
        "        self.tokenization = tokenization\n",
        "\n",
        "        tmp_annotations = pandas.read_pickle(annotation_path + \"refs(umd).p\")\n",
        "        tmp_instances = json.load(open(annotation_path + \"instances.json\", \"r\"))\n",
        "\n",
        "        annotations_dt = pandas.DataFrame.from_records(tmp_annotations) \\\n",
        "            .filter(items=[\"image_id\", \"split\", \"sentences\", \"ann_id\"])\n",
        "\n",
        "        instances_dt = pandas.DataFrame.from_records(tmp_instances['annotations'])\n",
        "\n",
        "        self.annotations = annotations_dt \\\n",
        "            .merge(instances_dt[[\"id\", \"bbox\", \"area\"]], left_on=\"ann_id\", right_on=\"id\") \\\n",
        "            .drop(columns=\"id\")\n",
        "\n",
        "        if split is not None:\n",
        "            self.annotations = self.__get_annotations_by_split(split.lower())\n",
        "\n",
        "    def getImage(self, sample):\n",
        "        id = sample['idx'][0].item()\n",
        "        item = self.annotations.iloc[id]\n",
        "        image = self.__getimage(item.image_id)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def getSentences(self, sample):\n",
        "        id = sample['idx'][0].item()\n",
        "        item = self.annotations.iloc[id]\n",
        "\n",
        "        return self.__extract_sentences(item.sentences)\n",
        "    \n",
        "    def showImage(self, train_features, train_bbox):\n",
        "        img = self.getImage(train_features)\n",
        "        img1 = ImageDraw.Draw(img)\n",
        "        img1.rectangle([(train_bbox[0].item(), train_bbox[1].item()), (train_bbox[2].item(), train_bbox[3].item())], outline =\"red\")\n",
        "        img.show()\n",
        "\n",
        "    def __get_annotations_by_split(self, split):\n",
        "        return self.annotations[self.annotations.split == split].reset_index()\n",
        "\n",
        "    def __getimage(self, id):\n",
        "        return Image.open(self.IMAGES_PATH + \"COCO_train2014_\" + str(id).zfill(12) + \".jpg\")\n",
        "\n",
        "    def __extract_sentences(self, sentences):\n",
        "        return [f\"a photo of {s['sent']}\" for s in sentences]\n",
        "\n",
        "    def __tokenize_sents(self, sentences):\n",
        "        return [self.tokenization(s) for s in sentences]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.annotations.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.annotations.iloc[idx]\n",
        "        image = self.__getimage(item.image_id)\n",
        "        sentences = self.__extract_sentences(item.sentences)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.tokenization:\n",
        "            sentences = self.__tokenize_sents(sentences)\n",
        "\n",
        "        sample = {'idx': idx, 'image': image, 'sentences': sentences}\n",
        "\n",
        "        return sample, item.bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhfTuC-_iPaL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import clip\n",
        "from PIL import Image\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def normalize(x: np.ndarray) -> np.ndarray:\n",
        "    x = x - x.min()\n",
        "    if x.max() > 0:\n",
        "        x = x / x.max()\n",
        "    return x\n",
        "\n",
        "def getAttMap(img, attn_map, blur=True):\n",
        "    if blur:\n",
        "        attn_map = gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n",
        "    attn_map = normalize(attn_map)\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
        "    attn_map = 1*(1-attn_map**0.7).reshape(attn_map.shape + (1,))*img + \\\n",
        "            (attn_map**0.7).reshape(attn_map.shape+(1,)) * attn_map_c\n",
        "    return attn_map\n",
        "\n",
        "def getCmap(img, attn_map, blur=True):\n",
        "    if blur:\n",
        "        attn_map = gaussian_filter(attn_map, 0.02*max(img.shape[:2]))\n",
        "    attn_map = normalize(attn_map)\n",
        "    cmap = plt.get_cmap('jet')\n",
        "    attn_map_c = np.delete(cmap(attn_map), 3, 2)\n",
        "    return attn_map_c\n",
        "\n",
        "def viz_attn(img, attn_map, blur=True):\n",
        "    _, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(img)\n",
        "    axes[1].imshow(getAttMap(img, attn_map, blur))\n",
        "    for ax in axes:\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()\n",
        "    \n",
        "def load_image(image, resize=None):\n",
        "    image = image.convert(\"RGB\")\n",
        "    if resize is not None:\n",
        "        image = image.resize((resize, resize))\n",
        "    return np.asarray(image).astype(np.float32) / 255.\n",
        "\n",
        "\n",
        "class Hook:\n",
        "    def __init__(self, module: nn.Module):\n",
        "        self.data = None\n",
        "        self.hook = module.register_forward_hook(self.save_grad)\n",
        "        \n",
        "    def save_grad(self, module, input, output):\n",
        "        self.data = output\n",
        "        output.requires_grad_(True)\n",
        "        output.retain_grad()\n",
        "        \n",
        "    def __enter__(self):\n",
        "        return self\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
        "        self.hook.remove()\n",
        "        \n",
        "    @property\n",
        "    def activation(self) -> torch.Tensor:\n",
        "        return self.data\n",
        "    \n",
        "    @property\n",
        "    def gradient(self) -> torch.Tensor:\n",
        "        return self.data.grad\n",
        "\n",
        "\n",
        "def gradCAM(model: nn.Module, input: torch.Tensor, target: torch.Tensor, layer: nn.Module) -> torch.Tensor:\n",
        "    if input.grad is not None:\n",
        "        input.grad.data.zero_()\n",
        "    \n",
        "    requires_grad = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        requires_grad[name] = param.requires_grad\n",
        "        param.requires_grad_(False)\n",
        "        \n",
        "    assert isinstance(layer, nn.Module)\n",
        "    with Hook(layer) as hook:        \n",
        "        output = model(input)\n",
        "        output.backward(target)\n",
        "\n",
        "        grad = hook.gradient.float()\n",
        "        act = hook.activation.float()\n",
        "    \n",
        "        alpha = grad.mean(dim=(2, 3), keepdim=True)\n",
        "        gradcam = torch.sum(act * alpha, dim=1, keepdim=True)\n",
        "        gradcam = torch.clamp(gradcam, min=0)\n",
        "\n",
        "    gradcam = F.interpolate(gradcam, input.shape[2:], mode='bicubic', align_corners=False)\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        param.requires_grad_(requires_grad[name])\n",
        "        \n",
        "    return gradcam\n",
        "\n",
        "\n",
        "class FeatureCouple:\n",
        "    def __init__(self, index, image_feature, sentence_feature, norm_image_feature, norm_sentence_feature):\n",
        "        self.index = index\n",
        "        self.image_feature = image_feature\n",
        "        self.sentence_feature = sentence_feature\n",
        "        self.similarity = norm_image_feature * norm_sentence_feature\n",
        "\n",
        "\n",
        "def getSalientEncodedFeatures(preprocessed_image, encoded_text, model):\n",
        "    with torch.no_grad():\n",
        "        encoded_image = model.encode_image(preprocessed_image).float()\n",
        "        norm_encoded_image = encoded_image / encoded_image.norm(dim=-1, keepdim=True)\n",
        "        norm_encoded_text = encoded_text / encoded_text.norm(dim=-1, keepdim=True)\n",
        "        original_sim = norm_encoded_text.cpu().numpy() @ norm_encoded_image.cpu().numpy().T\n",
        "    \n",
        "\n",
        "        features = []\n",
        "        for i in range(1024):\n",
        "            features.append(FeatureCouple(i, encoded_image[0][i].item(), encoded_text[0][i].item(), norm_encoded_image[0][i].item(), norm_encoded_text[0][i].item()))\n",
        "        features.sort(key=lambda x: x.similarity, reverse=True)\n",
        "\n",
        "        reconstruct_indexes = set()\n",
        "        reconstruct_sim = 0\n",
        "        for elem in features:\n",
        "            if reconstruct_sim / original_sim < 0.95:\n",
        "                reconstruct_sim += elem.similarity\n",
        "                reconstruct_indexes.add(elem.index)\n",
        "\n",
        "        for index in range(1024):\n",
        "            if index not in reconstruct_indexes:\n",
        "                encoded_text[0][index] = 0\n",
        "                encoded_image[0][index] = 0\n",
        "                norm_encoded_image[0][index] = 0\n",
        "                norm_encoded_text[0][index] = 0\n",
        "    \n",
        "    return encoded_image, encoded_text\n",
        "    return norm_encoded_image, norm_encoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPuuEeNau6ip"
      },
      "outputs": [],
      "source": [
        "def computeIntersection(fx1, fy1, fx2, fy2, sx1, sy1, sx2, sy2):\n",
        "    dx = min(fx2, sx2) - max(fx1, sx1)\n",
        "    dy = min(fy2, sy2) - max(fy1, sy1)\n",
        "    if (dx>=0) and (dy>=0):\n",
        "        area = dx*dy\n",
        "    else:\n",
        "        area = 0\n",
        "    return area\n",
        "\n",
        "def computeAccuracy(bbox, label):\n",
        "    intersection = computeIntersection(bbox[0], bbox[1], bbox[2], bbox[3],\n",
        "                                       label[0].item(), label[1].item(), label[0].item()+label[2].item(), label[1].item()+label[3].item())\n",
        "    area1 = (bbox[2]-bbox[0])*(bbox[3]-bbox[1])\n",
        "    area2 = label[2].item()*label[3].item()\n",
        "    union = area1 + area2 - intersection\n",
        "    return intersection / union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqnFGgnli9b2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "modelCLIP, preprocessCLIP = clip.load(\"RN50\", device=device)\n",
        "\n",
        "REFCOCOG_PATH = \"refcocog\"\n",
        "\n",
        "train_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"train\", transform=preprocessCLIP, tokenization=clip.tokenize)\n",
        "val_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"val\", transform=preprocessCLIP, tokenization=clip.tokenize)\n",
        "test_dataset = RefcocogDataset(REFCOCOG_PATH, split=\"test\", transform=preprocessCLIP, tokenization=clip.tokenize)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiaeyueHjS-v"
      },
      "outputs": [],
      "source": [
        "f = open(\"/content/drive/MyDrive/output.txt\", \"w\")\n",
        "\n",
        "def baseline(loader, dataset, modelCLIP, preprocessCLIP):\n",
        "    n_samples = 0\n",
        "    tot_accuracy = 0\n",
        "    blur = False\n",
        "    for data_features, data_bbox in loader:\n",
        "        sentences = dataset.getSentences(data_features)\n",
        "        minxs = []\n",
        "        minys = []\n",
        "        maxxs = []\n",
        "        maxys = []\n",
        "        for sent in sentences:\n",
        "            image = dataset.getImage(data_features)\n",
        "\n",
        "            preprocessed_image = preprocessCLIP(image).unsqueeze(0).to(device)\n",
        "\n",
        "            encoded_text = modelCLIP.encode_text(clip.tokenize([sent]).to(device)).float()\n",
        "            # encoded_image, encoded_text = getSalientEncodedFeatures(preprocessed_image, encoded_text, modelCLIP)\n",
        "\n",
        "            attn_map = gradCAM(modelCLIP.visual, preprocessed_image, encoded_text, getattr(modelCLIP.visual, \"layer4\"))\n",
        "            attn_map = attn_map.squeeze().detach().cpu().numpy()\n",
        "\n",
        "            #viz_attn(load_image(image, modelCLIP.visual.input_resolution), attn_map, blur)\n",
        "            cmap = getCmap(load_image(image, modelCLIP.visual.input_resolution), attn_map, blur)\n",
        "\n",
        "\n",
        "            red = torch.zeros((224, 224))\n",
        "\n",
        "            for i in range(cmap.shape[0]):\n",
        "                for j in range(cmap.shape[1]):\n",
        "                    for k in range(cmap.shape[2]):\n",
        "                        red[i][j] = cmap[i][j][0]\n",
        "\n",
        "            xs = []\n",
        "            ys = []\n",
        "\n",
        "            for i in range(224):\n",
        "                for j in range(224):\n",
        "                    if red[i][j] > 0.5:\n",
        "                        xs.append(j)\n",
        "                        ys.append(i)\n",
        "            \n",
        "            original_size = dataset.getImage(data_features).size\n",
        "            scaling_factor_x = original_size[0]/224\n",
        "            scaling_factor_y = original_size[1]/224\n",
        "\n",
        "            if(len(xs) > 0 and len(ys) > 0):\n",
        "                minxs.append(min(xs)*scaling_factor_x)\n",
        "                minys.append(min(ys)*scaling_factor_y)\n",
        "                maxxs.append(max(xs)*scaling_factor_x)\n",
        "                maxys.append(max(ys)*scaling_factor_y)\n",
        "\n",
        "        if(len(minxs) > 0 and len(minys) > 0 and len(maxxs) > 0 and len(maxys) > 0):\n",
        "            bbox = [sum(minxs)/len(minxs), sum(minys)/len(minys), sum(maxxs)/len(maxxs), sum(maxys)/len(maxys)]\n",
        "            accuracy = computeAccuracy(bbox, data_bbox)\n",
        "            tot_accuracy += accuracy\n",
        "            n_samples += 1\n",
        "            f.write(f'Image {n_samples:^8}/{len(dataset):^8}\\t{accuracy}\\n')\n",
        "            print(f'Image {n_samples:^8}/{len(dataset):^8}\\t{accuracy}')\n",
        "        else:\n",
        "            n_samples += 1\n",
        "            f.write(f'Image {n_samples:^8}/{len(dataset):^8}\\terror\\n')\n",
        "            print(f'Image {n_samples:^8}/{len(dataset):^8}\\terror')\n",
        "    f.write(f'Final accuracy\\t{tot_accuracy/n_samples}')\n",
        "    return tot_accuracy/n_samples\n",
        "\n",
        "print(baseline(val_dataloader, val_dataset, modelCLIP, preprocessCLIP))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PqAfhtr-mve"
      },
      "source": [
        "Best is using the encoded_text as target to backward on, results in 26% accuracy."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXo5iZvnhhTE+PHtBPICk9",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}