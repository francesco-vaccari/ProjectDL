{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "MEqmrzVdguM9",
        "Mrcm3GfAgh8U"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downloads"
      ],
      "metadata": {
        "id": "MEqmrzVdguM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bN2VCeF7b2p",
        "outputId": "f6ecce98-ca79-49b5-826a-2b56ec06eb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-5mkkff0f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-5mkkff0f\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=edd32c5851dba37dce17aa63facb07b7acc1319cb20baf5bc2527f7135c79ba7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m1_0jfwl/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
            "To: /content/refcocog.tar.gz\n",
            "100% 13.5G/13.5G [01:22<00:00, 163MB/s]\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m609.5/609.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!gdown 1xijq32XfEm6FPhUb7RsZYWHc2UuwVkiq\n",
        "!tar -xf /content/refcocog.tar.gz\n",
        "!pip install -qr https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1j-MGd-pbkppPiYYEYOyZ6vioTEoF-v_g\n",
        "!mv /content/adapters.py /usr/local/lib/python3.10/dist-packages/clip/\n",
        "!gdown 1C-h4h7pAkXR9-MhbBjXKLXvh9OwhXtOu\n",
        "!mv /content/model.py /usr/local/lib/python3.10/dist-packages/clip/\n",
        "!gdown 18kdAcm8P3GVgDp7GQfIaBg0L3ZeijK3v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHR113JU-FSa",
        "outputId": "de81be7e-e8ae-4214-991b-1f59ce7c6719"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j-MGd-pbkppPiYYEYOyZ6vioTEoF-v_g\n",
            "To: /content/adapters.py\n",
            "100% 5.79k/5.79k [00:00<00:00, 20.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C-h4h7pAkXR9-MhbBjXKLXvh9OwhXtOu\n",
            "To: /content/model.py\n",
            "100% 24.2k/24.2k [00:00<00:00, 46.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18kdAcm8P3GVgDp7GQfIaBg0L3ZeijK3v\n",
            "To: /content/RefcocogDataset.py\n",
            "100% 4.12k/4.12k [00:00<00:00, 14.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9EAuVfH-gpsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "from RefcocogDataset import RefcocogDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "PZ7YQJT88IL4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "Mrcm3GfAgh8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample(sample, bbox, idx=0):\n",
        "    print(f\"Sentence: {sample['sentences'][idx]}\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(sample['image'][idx].permute(1, 2, 0))\n",
        "    axes[1].imshow(bbox['gt'][idx])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_loss(map, bbox, idx, loss_map):\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    axs[0].imshow(map)\n",
        "    axs[1].imshow(bbox['gt'][idx])\n",
        "    axs[2].imshow(loss_map.reshape(14, 14))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "class BatchLossFunction(nn.Module):\n",
        "    def __init__(self, gamma=3.4, average=True):\n",
        "        super(BatchLossFunction, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.average = average\n",
        "\n",
        "    def forward(self, patch_tokens, out_text, gt):\n",
        "        loss = torch.zeros(1, requires_grad=True)\n",
        "        for idx in range(patch_tokens.shape[0]):\n",
        "            pt = patch_tokens[idx, 1:]\n",
        "            ot = out_text[idx, :].unsqueeze(0)\n",
        "            map = torch.zeros(196)\n",
        "\n",
        "            for i, token in enumerate(pt):\n",
        "                map[i] = 1 - torch.cosine_similarity(token, ot).item() # 1 - ... temporary fix\n",
        "\n",
        "            vector = torch.sigmoid(map)\n",
        "\n",
        "            gt_map = gt[idx]/255\n",
        "            gt_vector = gt_map.reshape(-1)\n",
        "\n",
        "            abs = torch.abs(vector - gt_vector)\n",
        "            log = -torch.log(1-abs)\n",
        "\n",
        "            # amplify the error of pixels that should belong to the object\n",
        "            log = log*(gt_vector*self.gamma+1)\n",
        "            loss = loss + torch.sum(log)\n",
        "\n",
        "        if self.average:\n",
        "            return (loss / patch_tokens.shape[0])\n",
        "        else:\n",
        "            return loss"
      ],
      "metadata": {
        "id": "Ytk1l4K9CuQu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "K8i5Pt4BgmE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def train_one_epoch(epoch_index, train_loader, model, criterion, optimizer, loop):\n",
        "    epoch_losses = []\n",
        "    for i, (samples, bbox) in enumerate(train_loader):\n",
        "        loop.set_postfix_str(f'Batch {i+1}/{len(train_loader)}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = samples['image'].to(device)\n",
        "        sentences = clip.tokenize(samples['sentences']).to(device)\n",
        "\n",
        "        out_image, out_text, patch_tokens, text_tokens = model.encode(images, sentences)\n",
        "\n",
        "        batch_loss = criterion(patch_tokens, out_text, bbox['gt'])\n",
        "\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_losses.append(batch_loss)\n",
        "\n",
        "    return torch.mean(torch.tensor(epoch_losses)).item()\n",
        "\n",
        "def train_loop(num_epochs, train_loader, model, criterion, optimizer, eval_loader = None):\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    run_path = 'runs/run_{}'.format(timestamp)\n",
        "    cmd = f'mkdir runs; mkdir runs/run_{timestamp}'\n",
        "    os.system(cmd)\n",
        "\n",
        "    best_eval_loss = float('inf')\n",
        "\n",
        "    loop = tqdm(range(num_epochs), desc=\"Training locator\", leave=True)\n",
        "    for epoch in loop:\n",
        "        model.train()\n",
        "        epoch_loss = train_one_epoch(epoch, train_loader, model, criterion, optimizer, loop)\n",
        "\n",
        "        if eval_loader is not None:\n",
        "            model.eval()\n",
        "            eval_losses = []\n",
        "            with torch.no_grad():\n",
        "                for samples, bbox in eval_loader:\n",
        "                    images = samples['image'].to(device)\n",
        "                    sentences = clip.tokenize(samples['sentences']).to(device)\n",
        "                    out_image, out_text, patch_tokens, text_tokens = model.encode(images, sentences)\n",
        "                    batch_loss = criterion(patch_tokens, out_text, bbox['gt'])\n",
        "                    eval_losses.append(batch_loss)\n",
        "\n",
        "                eval_loss = torch.mean(torch.tensor(eval_losses)).item()\n",
        "                torch.save(model.state_dict(), run_path + \"/last.pt\")\n",
        "\n",
        "                if eval_loss < best_eval_loss:\n",
        "                    best_eval_loss = eval_loss\n",
        "                    torch.save(model.state_dict(), run_path + \"/best.pt\")\n"
      ],
      "metadata": {
        "id": "LSNThhx_p_Rt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "ODRXzI06Br8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/16\") # only works with ViT-B/16\n",
        "model.init_adapters() # needed because state dict of clip does not contain adapters, goes before moving to gpu\n",
        "# model.load_parameters() # for when we have state dict of adapters trained, goes after adapters init\n",
        "model = model.to(device)\n",
        "\n",
        "model.freeze_for_training() # freezes all clip by putting requires_grad=False and then unfreezes adapters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_kEKuGlw-T5",
        "outputId": "0a098117-ad74-411b-cb1d-cd03e14fd942"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 335M/335M [00:02<00:00, 124MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # 32 should be possible\n",
        "train_dataset = RefcocogDataset(\"./refcocog\", split=\"train\", transform=preprocess)\n",
        "val_dataset = RefcocogDataset(\"./refcocog\", split=\"val\", transform=preprocess)\n",
        "test_dataset = RefcocogDataset(\"./refcocog\", split=\"test\", transform=preprocess)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # switch to shuffle True\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "HkBR3JvqFDxg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "num_epochs = 1\n",
        "\n",
        "criterion = BatchLossFunction(gamma=3.4, average=True) # keep 3.4 for now\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "train_loop(num_epochs, val_loader, model, criterion, optimizer, val_loader) # switch first val_loader to train_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3eBX3fxFG7m",
        "outputId": "890cc441-b9ef-4ec6-e65d-0333a49a4d9d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training locator: 100%|██████████| 1/1 [08:28<00:00, 508.10s/it, Batch 153/153]\n"
          ]
        }
      ]
    }
  ]
}